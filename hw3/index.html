<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Yongye Zhu, Shashank Anand</div>

		<br>

		Link to webpage: <a href="https://github.com/cal-cs184-student/sp25-hw3-fpsg-hw3">https://github.com/cal-cs184-student/sp25-hw3-fpsg-hw3</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/hw-webpages-fpsg-webpage">https://github.com/cal-cs184-student/hw-webpages-fpsg-webpage</a>
		
		<!-- <figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure> -->

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		In this homework, we mainly work on implementing efficient ray tracing algorithms. In order to accomplish this, we first build ray objects shot from the camera and implement ray-sphere and ray-triangle intersection. We then accelerate ray-object intersection by building a bounded box hierarchy. After that, we implement direct illumination by random sampling points on the hemisphere on ray-object intersections and optimize the radiance calculation by importance sampling. Then we implement indirect illumination by accumulating direct illumination with rays bounding from surfaces other than the light source. We use Russian roulette termination to terminate the bouncing lights. At the very end, we use adaptive sampling to sample more on points that are hard to converge. 
		<br>
		<br>
		Throughout the homework, the most important thing is to figure out what coordinate the vector lives in and do the proper transformation on that. Accumulating the correct numerical value when calculating the illumination is also important. 

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<p>
		Since we need to generate a ray from every pixel in screen space, we first generate the direction of the ray by transforming pixel location to camera space. Once we get the direction vector, we further transform the ray into world space by multiplying the camera-to-world vector. 
		</p>
		<p>
		Once we have a ray generation function, for each screen pixel, we randomly sample points within a grid and shoot a ray in that direction. Once we get the illumination of all the sampled locations, we average them and show the pixel color on the screen. 
		</p>
		<p>
		We then calculate the intersection of a ray and a shape (triangle/sphere) in world space. For triangles, we use the Möller Trumbore Algorithm to calculate time t and barycentric coordinates for the intersection inside the triangle. For the sphere, we solve the quadratic equation that calculates two intersection points and chooses one that is closest to the ray center while within the range of valid t.
		</p>
		<p>
		For triangle intersection cases, we use the Möller Trumbore Algorithm. Basically we model the point of intersection as a ray equation <b>p</b>= <b>o</b> + t<b>d</b> and the barycentric equation <b>p</b> = (1 - b_1 - b_2)<b>p1</b> + b1* <b>p2</b> + b2 * <b>p3</b>. So we have <b>o</b> + t<b>d</b> = (1 - b_1 - b_2)<b>p1</b> + b1* <b>p2</b> + b2 * <b>p3</b>. Since we have x,y,z three equations and t, b1, b2 three unknown variables. We can solve this system of equations. And we map them into the matrix form and solve it. 
		</p>
		<h3>Screenshots</h3>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
					  <img src="figs/part1/bunny.png" width="400px"/>
					  <figcaption>CBbunny.dae</figcaption>
					</td>
					<td style="text-align: center;">
					  <img src="figs/part1/CBspheres.png" width="400px"/>
					  <figcaption>CBspheres_lambertian.dae</figcaption>
					</td>
				  </tr>
			  <tr></tr>
			  <td style="text-align: center;">
				<img src="figs/part1/cow.png" width="400px"/>
				<figcaption>cow.dae</figcaption>
			  </td>
			  <td style="text-align: center;">
				  <img src="figs/part1/building.png" width="400px"/>
				  <figcaption>building.dae</figcaption>
				</td>
			  </tr>
			</table>

		<!-- <div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="cornell.png" width="400px"/>
				  <figcaption>Caption goes here.</figcaption>
				</td>
			  </tr>
			</table>
		</div> -->
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>
		We describe a better BVH construction in the extra credit section. Here we only talk about the naive implementation. 
		We construct all the primitives within a range [start, end) in a single bounding box. If the number of primitives fits within <i>max_leaf_size</i>, we terminate. If not, we find the longest axis of the bounding box and split it in the median location. We then split in the median location and recursively construct the child bounding box. 
		</p>

		<h3>Screenshots</h3>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
					  <img src="figs/part2/beast.png" width="400px"/>
					  <figcaption>beast.dae</figcaption>
					</td>
					<td style="text-align: center;">
					  <img src="figs/part2/lucy.png" width="400px"/>
					  <figcaption>CBlucy.dae</figcaption>
					</td>
				  </tr>
			  <tr></tr>
			  <td style="text-align: center;">
				<img src="figs/part2/mp.png" width="400px"/>
				<figcaption>maxplanck.dae</figcaption>
			  </td>
			  <td style="text-align: center;">
				  <img src="figs/part2/peter.png" width="400px"/>
				  <figcaption>peter.dae</figcaption>
				</td>
			  </tr>
			</table>
			<p>
			We measure the speedup for several meshes after implementing the BVH. For each meshes, we run with 8 threads and pixel size 800x600. The table belows shows that even implementing a naive BVH scheme can massive speed up the ray-scene intersection computation. 
			</p>
			

			<table border="1" width="80%" style="font-size: 24px;">
				<tr>
					<th>Mesh</th>
					<th>w/ BVH</th>
					<th>w/o BVH</th>
				</tr>
				<tr>
					<td>cow.dae</td>
					<td>0.17s</td>
					<td>8.08s</td>
				</tr>
				<tr>
					<td>maxplanck.dae</td>
					<td>0.49s</td>
					<td>93.8s</td>
				</tr>
				<tr>
					<td>CBlucy.dae</td>
					<td>1.65s</td>
					<td>241.58s</td>
				</tr>
				<caption><strong>Comparison of rendering time with BVH</strong></caption>
			</table>

		


		<h2>Part 3: Direct Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Adaptive sampling attempts to generate noise free images by distributing samples over pixels depending on their convergence rate. We use more samples for difficult regions and fewer samples for "easier" regions.
		In our implementation, we modify the raytrace_pixel function. Inside the sampling loop for each pixel, we add a termination check. This termination check is triggered every samplesPerBatch pixels, and terminates out of the loop early if the desired convergence rate has been achieved. We check for convergence using the described method in the Part 5 page: using the variance of samples so far and checking that \(1.96*\sqrt{\frac{\sigma}{n}} \leq maxTolerance*\mu\). When terminating early, we use the value of i as number of samples evaluated.
		
		We render 2048 samples of the bunny model and the wall-e model with the following commands:<br/>
		<code> ./pathtracer -t 8 -s 2048 -a 64 0.05 -l 1 -m 5 -r 480 360 -f bunny.png ../dae/sky/CBbunny.dae </code> <br/>

		<code> ./pathtracer -t 8 -s 2048 -a 64 0.05 -l 1 -m 5 -r 480 360 -f wall-e.png ../dae/sky/wall-e.dae </code> <br/>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="figs/part5/bunny.png" width="400px"/>
				  <figcaption>Bunny with 2048 samples</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="figs/part5/bunny_rate.png" width="400px"/>
				  <figcaption>Bunny sample heatmap</figcaption>
				</td>
			  </tr>
			  <tr></tr>
				<td style="text-align: center;">
				  <img src="figs/part5/wall-e.png" width="400px"/>
				  <figcaption>Wall-E with 2048 samples</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="figs/part5/wall-e_rate.png" width="400px"/>
				  <figcaption>Wall-E sample heatmap</figcaption>
				</td>
			  </tr>
			</table>

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		<h3> Extra Credit 1: Better BVH splitting using surface area heuristic</h3>
			To improve performance, we implemented a better BVH splitting algorithm using the surface area heuristic.
			For each axis, we evaluate the best partition among all the primitives by iterating through the sorted list of primitives and calculating the cost of splitting at each primitive. The cost of splitting is equal to the cost of intersecting the left and right boxes. The cost of intersecting a box is equal to the probability of intersecting this box (proportional to the surface area) times the number of primitives in this box.
			We minimize this cost for each axis, and pick the best cost among all three axes.
			We skip straight to path tracing the 2048 sample bunny from Part 5:
			<code> ./pathtracer -t 8 -s 2048 -a 64 0.05 -l 1 -m 5 -r 480 360 -f bunny.png ../dae/sky/CBbunny.dae </code>
			We see a significant speedup in rendering time, going from 237 seconds without SAH splitting, to 154 seconds with SAH splitting, an approximately 35% improvement.
		
		<h3> Extra Credit 2 + 3: Optimizations </h3>
			We perform the following optimizations: Replace recursion with stack traversal in bbox construction, z culling when sampling lights, and more optimized bbox and bvh tests. We mention them as 2 extra credits as they are two separate items in the extra credit list.
			As above, we run the 2048 sample bunny from Part 5. We get a good spedup, rendering time going from 154 seconds with SAH splitting to 130 seconds with SAH splitting and optimizations, an approximately 15% improvement.

		<h3> Extra Credit 4: Jittered Sampling </h3>
			We create a new class of 2D Sampler called the JitteredSampler2D. During instantiation, an NxN grid is created, and we randomly sample a point inside this grid and store it. When calling sample(), we uniformly randomly generate i,j between [0,1] and use \((floor(i*N), floor(j*N))\) as the grid to pick the sample point from.
			This provides us with a better distributed set of samples.

			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
				  <tr>
					<td style="text-align: center;">
					  <img src="figs/part6/bunny.png" width="400px"/>
					  <figcaption>Uniform random pixel sampling.</figcaption>
					</td>
					<td style="text-align: center;">
					  <img src="figs/part6/bunny_jit.png" width="400px"/>
					  <figcaption>Jittered sampling</figcaption>
					</td>
				  </tr>
				</table>
			</div>

			The difference in the pictures above are not very apparent, but we can see upon zooming in that jittered sampling provides better fidelity and less aliasing along the edges of the bunny. Particularly, the ears and the eyes show noticeable difference.
		</div>
	</body>
</html>